{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final images data creation\n",
    "\n",
    "This notebook contains two parts:\n",
    "1. Uploading all users images by labeled classes\n",
    "2. Collecting information about uploaded images in dataframe\n",
    "\n",
    "In the first part code for images uploading and storing by users and prepared category label is provided.\n",
    "\n",
    "In the second part final function for merges creation from these images is provided. This data formed the training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading all users images by labeled classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = pd.read_csv('all_labeled_merges.csv')\n",
    "merges = merges[['merge_name', 'user_id', 'images', 'class_lbl']]\n",
    "\n",
    "# Fix arrays after uploading from csv (convert them from strings)\n",
    "merges['images'] = list(map(ast.literal_eval, merges['images'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the merges lists of images urls into one list of all images urls when we load images in the first step\n",
    "merge_list_list = list(merges.merge_name)\n",
    "images_list = list(merges.images)\n",
    "image_download_list = []\n",
    "for i in range(len(merge_list_list)):\n",
    "    k = 0\n",
    "    for j in images_list[i]:\n",
    "        new_name = merge_list_list[i] + '__%i' %k\n",
    "        image_download_list.append([merge_list_list[i], new_name, j])\n",
    "        k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = pd.DataFrame(image_download_list,columns=['merge_name','image_name','im_url', 'user_id', 'class_lbl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(zip(df_images.image_name, df_images.im_url, df_images.class_lbl, df_images.user_id))\n",
    "group_len = 115\n",
    "images_groups = list(sliced(images, group_len)) # for multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for images downloading\n",
    "def images_downloader(posts, main_path):\n",
    "    for i in range(len(posts[:])): \n",
    "        # In the case of the second stage of loading, when images are downloaded to class folders and users, \n",
    "        # the parameter 'posts' length should be 4 and contain the class_lbl and user_id lists\n",
    "        if len(posts) == 4:\n",
    "            main_path = main_path + '/' + posts[i][2] + '/' + posts[i][3] + '/'\n",
    "        try:\n",
    "            urllib.request.urlretrieve(posts[i][1], myPath + posts[i][0])\n",
    "        except:\n",
    "            None\n",
    "            \n",
    "# main function for parallel downloading images by prepared urls\n",
    "def parallel_downloading(images_groupes, it=0, step=10, n_processes=10)\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    while it <= len(images_groups1) - step:\n",
    "        print(it)\n",
    "        if __name__ == '__main__':\n",
    "            pool = Pool(n_processes) # set processes number    \n",
    "            result = pool.map(images_downloader, images_groups[it : it + step])\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        it = it + step\n",
    "        print('time - %f' %(timeit.default_timer() - start))\n",
    "        \n",
    "    # download last portion of data\n",
    "    if __name__ == '__main__':\n",
    "        pool = Pool(n_processes) # set processes number     \n",
    "        result = pool.map(images_downloader, images_groups[it:])\n",
    "        pool.close()\n",
    "        pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# start downloading images to the <images_data_path> directory by classes by user ids\n",
    "parallel_downloading(images_groups, '<final_images_data_path>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting information about uploaded images in dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial state:** all images should be placed in the folders with their user id, which should be placed in folders with the class label to which they belong.\n",
    "\n",
    "**Example of folder structure:**\n",
    "\n",
    "* uploaded_images\n",
    "    * brand\n",
    "        * Potapova.a\n",
    "            * image1\n",
    "            * image2\n",
    "            * ...\n",
    "    * food\n",
    "        * best_kitchen_ever\n",
    "            * image1\n",
    "            * image2\n",
    "            * ...\n",
    "\n",
    "The main directory `uploaded_images` is passed as the input of the`create_csv_for_uploaded_images` function.\n",
    "\n",
    "Below is a list of all classes. All classes subfolders in the main directory should have these names too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all 9 classes labels\n",
    "lbl_classes = ['brand', 'lifestyle', 'thematic', 'food', \n",
    "               'bad_brand', 'bad_lifestyle', 'bad_thematic', 'bad_food', 'bad_beauty_services']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for collecting information about uploaded images to a DataFrame\n",
    "# It returns a dataframe that lists information of all images from all classes in a provided main directory\n",
    "# second and further carousel images will not be placed in the result dataframe\n",
    "def create_csv_for_uploaded_images(imgs_directory):\n",
    "    all_imgs_df = pd.DataFrame(columns=['image_name', 'username', 'class', 'is_carousel', 'carousel_num'])\n",
    "    for lbl_class in lbl_classes:\n",
    "        class_posts = os.listdir(imgs_directory + '/' + lbl_class)\n",
    "        # collect info about all all images names and user\n",
    "        users = []\n",
    "        images = []\n",
    "        for i in class_posts:\n",
    "            images = images + os.listdir(imgs_directory + '/' + lbl_class + '/%s' %i)\n",
    "            users = users + [i] * len(os.listdir(imgs_directory + '/' + lbl_class + '/%s' %i))\n",
    "            \n",
    "        df = pd.DataFrame(images, columns=['image_name'])\n",
    "        df['username'] = users\n",
    "        df['class'] = lbl_class\n",
    "        \n",
    "        # check if the image is second or further in the carousel\n",
    "        df.loc[df.image_name.str.count('(\\_[0-9]{1}\\.[jpg]{3})')>0,'is_carousel'] = 1\n",
    "        df.loc[df.is_carousel.isnull(),'is_carousel'] = 0\n",
    "        \n",
    "        df.loc[df.is_carousel==1,'carousel_num'] = df[df.is_carousel==1]['image_name'].apply(lambda x: int(x.split('_')[-1].replace('.jpg','')))\n",
    "        \n",
    "        df['carousel_num'] = df['carousel_num'].fillna(-1)\n",
    "        \n",
    "        # save only first carousel images or images from posts without carousel\n",
    "        df_final = df[df.carousel_num <= 1].reset_index(drop=True)\n",
    "        all_imgs_df = pd.concat([all_imgs_df, df_final], ignore_index=True)\n",
    "        \n",
    "    return all_imgs_df        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_csv_for_uploaded_images('<final_images_data_path>')\n",
    "df.to_csv('final_images_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merges creating for the collected images\n",
    "\n",
    "Cases when images are not square are also handled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df = pd.read_csv('final_images_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(img):\n",
    "    # for cases when images are not squared we crop them\n",
    "    crop_area1 = (0, (img.height - img.width) // 2, img.width, img.height - (img.height - img.width) // 2)\n",
    "    crop_area2 = (-(img.height - img.width) // 2, 0, img.width + (img.height - img.width) // 2, img.height)\n",
    "    \n",
    "    if img.height > img.width:\n",
    "        img = img.crop(crop_area1)\n",
    "    elif img.height < img.width:\n",
    "        img = img.crop(crop_area2)\n",
    "    return img\n",
    "\n",
    "# function for 9 images concatenation in the form of 3x3 square\n",
    "def concat_imgs(imgs_lst):\n",
    "    img_h = imgs_lst[0].height\n",
    "    img_w = imgs_lst[0].width\n",
    "    dst = Image.new('RGB', (3 * img_w, 3 * img_h))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            dst.paste(imgs_lst[i * 3 + j], (img_w * j, img_h * i))\n",
    "    return dst\n",
    "\n",
    "# main fuction for images merging\n",
    "def merge_images(parameters, class_dir, merges_name):\n",
    "    uploaded_imgs = []\n",
    "    uploaded_fl = 0\n",
    "    for img_name in parameters:\n",
    "        try:\n",
    "            img = Image.open('<uploaded_images_by_classes_path>/' + class_dir + img_name)\n",
    "        except(FileNotFoundError):\n",
    "            uploaded_fl = -1\n",
    "            break\n",
    "        img = crop_image(img)\n",
    "        img = img.resize((150, 150))\n",
    "        uploaded_imgs.append(img)\n",
    "    if uploaded_fl == 0:\n",
    "        image = concat_imgs(uploaded_imgs)\n",
    "        image.save('<final_merges_directory_path>/' + class_dir.split('/')[0] + '/' + merges_name + '.jpg')\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20 # we choose 9 random images for each 20 sequential images\n",
    "n_total = 500 # maximum total number of final merges for 1 user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for merges creation (from prepared uploaded images, using prepared csv with info from above) \n",
    "# and saving merges in the main directory by classes labels\n",
    "for user_id in list(set(posts_df.username.values)):\n",
    "    all_user_images = posts_df[posts_df['username'] == user_id].image_name.values\n",
    "    class_lbl = posts_df[posts_df['username'] == user_id]['class'].values[0]\n",
    "    class_dir = class_lbl + '/' + user_id + '/'\n",
    "    \n",
    "    n_batches = int(np.ceil(len(all_user_images) / batch_size))\n",
    "    n_per_batch = max(9, int(np.floor(n_total / n_batches))) # fix situation, when n_per_batch is less than 9\n",
    "    \n",
    "    for start_id in range(n_batches):\n",
    "        images = all_user_images[start_id : start_id + n_per_batch]\n",
    "        it = 0\n",
    "        n_tries = 0 # for situations, when it is not possible to create merge we try with different image names for 5 times \n",
    "        # and after that move to the next portion of images\n",
    "        while (it < n_per_batch) and (n_tries <= 5):\n",
    "            random.shuffle(images)\n",
    "            images_new = images[:9]\n",
    "            if merge_images(images_new, class_dir,\n",
    "                            str(user_id) + '_merge_' + str(start_id * n_per_batch + it + 1)):\n",
    "                n_tries += 1\n",
    "                if n_tries == 5:\n",
    "                    continue\n",
    "            it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result we receive final data with the image merges for all nine classed. This data was used by us to train and validate the neural network model. "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
